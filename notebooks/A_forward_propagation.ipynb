{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing forward propagation through a simple neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "In this notebook, you will implement forward propagation through a simple fully connected neural network.\n",
    "\n",
    "Before diving into the code, let's briefly review some key concepts and definitions. These are summarized in the table below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Key Concepts\n",
    "\n",
    "| **Concept**                | **Description** |\n",
    "|---------------------------|-----------------|\n",
    "| **Neural Network (NN)**   | A computational model inspired by the human brain, composed of layers of interconnected nodes (neurons) that transform input data to learn patterns. |\n",
    "| **Fully Connected Network (FCN)** | A type of neural network in which every neuron in one layer is connected to every neuron in the next layer. It is the simplest architecture used for function approximation, classification, and regression tasks. |\n",
    "| **Layer**                 | A collection of neurons that processes input data. Layers can be input, hidden, or output layers. |\n",
    "| **Neuron (Unit)**         | A basic computation unit in a network. Each neuron performs a weighted sum of its inputs followed by an activation function. |\n",
    "| **Weight Matrix ($\\mathbf{W}$)** | A matrix that stores the trainable parameters between layers. Each element represents the strength of the connection between two neurons. |\n",
    "| **Bias Vector ($\\mathbf{b}$)**   | A vector added to the weighted sum to provide flexibility and improve learning. Each layer typically has its own bias vector. |\n",
    "| **Forward Propagation**   | The process of computing outputs from inputs by passing data through the network's layers using weights, biases, and activation functions. |\n",
    "| **Linear Transformation** | The operation $ \\mathbf{z} = \\mathbf{W}\\mathbf{x} + \\mathbf{b} $, used to combine inputs using learnable parameters. |\n",
    "| **Activation Function**   | A non-linear function applied to the linear output of a neuron, enabling the network to learn complex patterns. |\n",
    "| **ReLU (Rectified Linear Unit)** | An activation function defined as $$ \\text{ReLU}(z) = \\max(0, z) $$ introducing non-linearity while maintaining computational efficiency. |\n",
    "| **Softmax Function**      | Converts a vector of scores into a probability distribution:  $$ \\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}} $$ Used in the output layer for classification. |\n",
    "| **Hidden Layer**          | An intermediate layer between the input and output. Hidden layers allow the network to learn internal representations. |\n",
    "| **Output Layer**          | The final layer that produces predictions. In classification tasks, this often applies softmax to yield class probabilities. |\n",
    "| **Predicted Class ($\\hat{y}$)** | The class with the highest predicted probability: $$ \\hat{y} = \\arg\\max_i \\mathbf{a}^{[2]}_i $$ |\n",
    "| **Dimensionality**        | Refers to the number of features or units in each layer. Shapes must match for matrix operations to be valid. |\n",
    "| **Matrix Multiplication** | The core operation in forward propagation, denoted $ \\mathbf{W}\\mathbf{x} $. Ensures linear combination of inputs. |\n",
    "| **Vectorization**         | Expressing computations using matrix and vector operations to improve efficiency and code readability. |\n",
    "| **Batch Processing**      | Feeding multiple inputs at once (as a matrix of inputs) to leverage parallelism during forward passes. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Fully connected network (FCN) architecture\n",
    "\n",
    "In this exercise, we will implement a **fully connected network (FCN)**, also known as a feedforward neural network. This type of network consists of layers where each neuron in one layer is connected to every neuron in the next layer.\n",
    "\n",
    "The architecture of an FCN is defined by the following components:\n",
    "\n",
    "- **Input dimension**: the number of features in the input vector.\n",
    "- **Number of hidden layers**: intermediate layers between input and output, responsible for learning internal representations.\n",
    "- **Neurons per hidden layer**: the number of units in each hidden layer.\n",
    "- **Activation functions**: non-linear transformations applied after each layer to allow the network to learn complex patterns.\n",
    "- **Output dimension**: the number of classes (for classification) or output values (for regression).\n",
    "\n",
    "The code below generates an FCN with an input dimension of 2, one hidden layer with three neurons, and an output dimension of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import NeuralNetwork\n",
    "\n",
    "nn_dim = [2, 3, 2]\n",
    "nn = NeuralNetwork(nn_dim)\n",
    "nn.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try changing the architecture by modifying the nn_dim variable. Create a network with an input dimension of 3, two hidden layers with 5 neurons each, and an output dimension of 2. Then, draw the resulting architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change the architecture by changing the nn_dim variable.\n",
    "\n",
    "from exercise_validator import validate_nn_architecture\n",
    "\n",
    "nn_dim = None # You can change the architecture by changing the nn_dim variable\n",
    "nn = NeuralNetwork(nn_dim)\n",
    "nn.draw()\n",
    "\n",
    "# Validate the solution\n",
    "is_valid, message = validate_nn_architecture(nn_dim)\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE SOLUTION — NOT VISIBLE TO LEARNERS\n",
    "# This cell contains the reference solution and validation.\n",
    "# It should be used for instructor/testing purposes only.\n",
    "\n",
    "from exercise_validator import validate_nn_architecture\n",
    "\n",
    "nn_dim = [3, 5, 5, 2] # You can change the architecture by changing the nn_dim variable\n",
    "nn = NeuralNetwork(nn_dim)\n",
    "nn.draw()\n",
    "\n",
    "# Validate the solution\n",
    "is_valid, message = validate_nn_architecture(nn_dim)\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Forward Propagation Through an FCN\n",
    "\n",
    "Forward propagation is the process of passing the input through the network, layer by layer, applying linear transformations followed by activation functions, until the final output is produced.\n",
    "\n",
    "Formally, for a fully connected network with one hidden layer, the computations proceed as follows:\n",
    "\n",
    "- Let the input vector be $\\mathbf{x} \\in \\mathbb{R}^{n_0}$\n",
    "- Let the weight matrix and bias vector for layer 1 be:\n",
    "  - $\\mathbf{W}^{[1]} \\in \\mathbb{R}^{n_1 \\times n_0}$\n",
    "  - $\\mathbf{b}^{[1]} \\in \\mathbb{R}^{n_1}$\n",
    "- Let the weight matrix and bias vector for the output layer be:\n",
    "  - $\\mathbf{W}^{[2]} \\in \\mathbb{R}^{n_2 \\times n_1}$\n",
    "  - $\\mathbf{b}^{[2]} \\in \\mathbb{R}^{n_2}$\n",
    "\n",
    "The forward propagation steps are:\n",
    "\n",
    "1. **First layer linear transformation**  \n",
    "   $$\n",
    "   \\mathbf{z}^{[1]} = \\mathbf{W}^{[1]} \\mathbf{x} + \\mathbf{b}^{[1]}, \\quad \\mathbf{z}^{[1]} \\in \\mathbb{R}^{n_1}\n",
    "   $$\n",
    "\n",
    "2. **First layer activation (e.g., ReLU)**  \n",
    "   $$\n",
    "   \\mathbf{a}^{[1]} = \\text{ReLU}(\\mathbf{z}^{[1]}), \\quad \\mathbf{a}^{[1]} \\in \\mathbb{R}^{n_1}\n",
    "   $$\n",
    "\n",
    "3. **Output layer linear transformation**  \n",
    "   $$\n",
    "   \\mathbf{z}^{[2]} = \\mathbf{W}^{[2]} \\mathbf{a}^{[1]} + \\mathbf{b}^{[2]}, \\quad \\mathbf{z}^{[2]} \\in \\mathbb{R}^{n_2}\n",
    "   $$\n",
    "\n",
    "4. **Output activation (e.g., softmax for classification)**  \n",
    "   $$\n",
    "   \\mathbf{a}^{[2]} = \\text{softmax}(\\mathbf{z}^{[2]}), \\quad \\mathbf{a}^{[2]} \\in \\mathbb{R}^{n_2}\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "**Example:**  \n",
    "Consider an FCN with:\n",
    "\n",
    "- Input dimension: $n_0 = 3$  \n",
    "- Hidden layer size: $n_1 = 4$  \n",
    "- Output dimension: $n_2 = 2$  \n",
    "\n",
    "The corresponding dimensions of the parameters and intermediate computations will be:\n",
    "\n",
    "- $\\mathbf{x} \\in \\mathbb{R}^{3}$  \n",
    "- $\\mathbf{W}^{[1]} \\in \\mathbb{R}^{4 \\times 3}$, $\\mathbf{b}^{[1]} \\in \\mathbb{R}^{4}$  \n",
    "- $\\mathbf{z}^{[1]}, \\mathbf{a}^{[1]} \\in \\mathbb{R}^{4}$  \n",
    "- $\\mathbf{W}^{[2]} \\in \\mathbb{R}^{2 \\times 4}$, $\\mathbf{b}^{[2]} \\in \\mathbb{R}^{2}$  \n",
    "- $\\mathbf{z}^{[2]}, \\mathbf{a}^{[2]} \\in \\mathbb{R}^{2}$\n",
    "\n",
    "This setup results in a 2-dimensional output vector that can be interpreted as class probabilities in a binary classification task.\n",
    "\n",
    "Let's assign the following values:\n",
    "\n",
    "**Input vector:**\n",
    "$$\n",
    "\\mathbf{x} =\n",
    "\\begin{bmatrix}\n",
    "1.0 \\\\\n",
    "2.0 \\\\\n",
    "-1.0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Weights and biases for the hidden layer:**\n",
    "$$\n",
    "\\mathbf{W}^{[1]} =\n",
    "\\begin{bmatrix}\n",
    "0.2 & -0.4 & 0.1 \\\\\n",
    "0.7 & 0.3 & -0.2 \\\\\n",
    "-0.5 & 0.6 & 0.4 \\\\\n",
    "0.1 & -0.1 & 0.2\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "\\mathbf{b}^{[1]} =\n",
    "\\begin{bmatrix}\n",
    "0.0 \\\\\n",
    "0.1 \\\\\n",
    "-0.2 \\\\\n",
    "0.05\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let's compute the first linear transformation using **numpy**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([1.0, 2.0, -1.0])\n",
    "W1 = np.array([[0.2, -0.4, 0.1], [0.7, 0.3, -0.2], [-0.5, 0.6, 0.4], [0.1, -0.1, 0.2]])\n",
    "b1 = np.array([0.0, 0.1, -0.2, 0.05])\n",
    "z1 = np.dot(W1, x) + b1\n",
    "print(z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In matricial notation we have\n",
    "\n",
    "$$\n",
    "\\mathbf{z}^{[1]} = \\mathbf{W}^{[1]} \\mathbf{x} + \\mathbf{b}^{[1]} =\n",
    "\\begin{bmatrix}\n",
    "(0.2)(1.0) + (-0.4)(2.0) + (0.1)(-1.0) \\\\\n",
    "(0.7)(1.0) + (0.3)(2.0) + (-0.2)(-1.0) \\\\\n",
    "(-0.5)(1.0) + (0.6)(2.0) + (0.4)(-1.0) \\\\\n",
    "(0.1)(1.0) + (-0.1)(2.0) + (0.2)(-1.0)\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0.0 \\\\\n",
    "0.1 \\\\\n",
    "-0.2 \\\\\n",
    "0.05\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "-0.7 \\\\\n",
    "1.6 \\\\\n",
    "0.1 \\\\\n",
    "-0.25\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Apply the ReLU activation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = np.maximum(0, z1)\n",
    "print(a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{a}^{[1]} = \\text{ReLU}(\\mathbf{z}^{[1]}) =\n",
    "\\begin{bmatrix}\n",
    "0.0 \\\\\n",
    "1.6 \\\\\n",
    "0.1 \\\\\n",
    "0.0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Weights and biases for the output layer:**\n",
    "$$\n",
    "\\mathbf{W}^{[2]} =\n",
    "\\begin{bmatrix}\n",
    "0.5 & -0.3 & 0.8 & -0.1 \\\\\n",
    "-0.4 & 0.6 & 0.1 & 0.2\n",
    "\\end{bmatrix},\n",
    "\\quad\n",
    "\\mathbf{b}^{[2]} =\n",
    "\\begin{bmatrix}\n",
    "0.0 \\\\\n",
    "0.05\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let's compute the output layer transformation using **numpy**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = np.array([[0.5, -0.3, 0.8, -0.1], [-0.4, 0.6, 0.1, 0.2]])\n",
    "b2 = np.array([0.0, 0.05])\n",
    "z2 = np.dot(W2, a1) + b2\n",
    "print(z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In matricial notation we have\n",
    "\n",
    "$$\n",
    "\\mathbf{z}^{[2]} = \\mathbf{W}^{[2]} \\mathbf{a}^{[1]} + \\mathbf{b}^{[2]} =\n",
    "\\begin{bmatrix}\n",
    "(0.5)(0.0) + (-0.3)(1.6) + (0.8)(0.1) + (-0.1)(0.0) \\\\\n",
    "(-0.4)(0.0) + (0.6)(1.6) + (0.1)(0.1) + (0.2)(0.0)\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0.0 \\\\\n",
    "0.05\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "-0.4 \\\\\n",
    "1.02\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Apply the softmax function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = np.exp(z2) / np.sum(np.exp(z2))\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{a}^{[2]} = \\text{softmax}(\\mathbf{z}^{[2]}) =\n",
    "\\frac{1}{e^{-0.4} + e^{1.02}} \\cdot\n",
    "\\begin{bmatrix}\n",
    "e^{-0.4} \\\\\n",
    "e^{1.02}\n",
    "\\end{bmatrix}\n",
    "\\approx\n",
    "\\begin{bmatrix}\n",
    "0.195 \\\\\n",
    "0.805\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement Your Own Fully Connected Network (FCN)\n",
    "\n",
    "In this section, you will implement a simple **fully connected network (FCN)** using a class-based structure in Python. The architecture of the network is as follows:\n",
    "\n",
    "1. **Input Layer**: 2 units, accepting a feature vector $\\mathbf{x} \\in \\mathbb{R}^2$\n",
    "2. **Hidden Layer**: 2 units with ReLU activation  \n",
    "   - Linear transformation: $\\mathbf{z}^{[1]} = \\mathbf{W}^{[1]} \\mathbf{x} + \\mathbf{b}^{[1]}$  \n",
    "   - Activation: $\\mathbf{a}^{[1]} = \\text{ReLU}(\\mathbf{z}^{[1]})$\n",
    "3. **Output Layer**: 2 units with softmax activation  \n",
    "   - Linear transformation: $\\mathbf{z}^{[2]} = \\mathbf{W}^{[2]} \\mathbf{a}^{[1]} + \\mathbf{b}^{[2]}$  \n",
    "   - Activation: $\\mathbf{y} = \\text{softmax}(\\mathbf{z}^{[2]})$\n",
    "\n",
    "---\n",
    "\n",
    "### ✍️ Task\n",
    "\n",
    "Your task is to complete the class `SimpleFCN` by filling in the missing components. The class will have two main responsibilities:\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. **Assigning Network Parameters**\n",
    "\n",
    "Manually assign values to the weights and biases of the network using the following:\n",
    "\n",
    "- $\\mathbf{W}^{[1]} \\in \\mathbb{R}^{2 \\times 2}, \\quad \\mathbf{b}^{[1]} \\in \\mathbb{R}^{2}$\n",
    "- $\\mathbf{W}^{[2]} \\in \\mathbb{R}^{2 \\times 2}, \\quad \\mathbf{b}^{[2]} \\in \\mathbb{R}^{2}$\n",
    "\n",
    "**Use these fixed values:**\n",
    "\n",
    "$$\n",
    "\\mathbf{W}^{[1]} =\n",
    "\\begin{bmatrix}\n",
    "0.2 & -0.4 \\\\\n",
    "0.7 & 0.3\n",
    "\\end{bmatrix}, \\quad\n",
    "\\mathbf{b}^{[1]} =\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\\n",
    "0.0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{W}^{[2]} =\n",
    "\\begin{bmatrix}\n",
    "0.5 & -0.3 \\\\\n",
    "-0.4 & 0.6\n",
    "\\end{bmatrix}, \\quad\n",
    "\\mathbf{b}^{[2]} =\n",
    "\\begin{bmatrix}\n",
    "0.0 \\\\\n",
    "0.05\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Performing Forward Propagation**\n",
    "\n",
    "You will implement the full forward pass from input $\\mathbf{x}$ to output $\\mathbf{y}$ by combining the following components:\n",
    "\n",
    "- A **linear transformation function**\n",
    "- A **ReLU activation function**\n",
    "- A **softmax function**\n",
    "- A `forward()` method that chains all steps together\n",
    "\n",
    "---\n",
    "\n",
    "### 🧩 Components to Implement\n",
    "\n",
    "1. **Parameter Initialization**  \n",
    "   Use the provided values for the weights and biases of both layers.\n",
    "\n",
    "2. **Linear Forward**  \n",
    "   Implements the affine transformation:\n",
    "   $$\n",
    "   \\mathbf{z} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}\n",
    "   $$\n",
    "\n",
    "3. **ReLU Activation**  \n",
    "   Applies the ReLU function element-wise:\n",
    "   $$\n",
    "   \\text{ReLU}(z_i) = \\max(0, z_i)\n",
    "   $$\n",
    "\n",
    "4. **Softmax Activation**  \n",
    "   Converts raw scores into probabilities:\n",
    "   $$\n",
    "   \\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n",
    "   $$\n",
    "\n",
    "5. **Forward Pass**  \n",
    "   Combine all components to compute the network output:\n",
    "   $$\n",
    "   \\mathbf{y} = \\text{softmax} \\left( \\mathbf{W}^{[2]} \\cdot \\text{ReLU}\\left( \\mathbf{W}^{[1]} \\mathbf{x} + \\mathbf{b}^{[1]} \\right) + \\mathbf{b}^{[2]} \\right)\n",
    "   $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the implementation of the SimpleFCN class by filling in the missing methods\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class SimpleFCN:\n",
    "    def __init__(self):\n",
    "        # Assign fixed weights and biases for layer 1\n",
    "        self.W1 = None # TODO: Assign the weights for layer 1\n",
    "        self.b1 = None # TODO: Assign the biases for layer 1\n",
    "\n",
    "        # Assign fixed weights and biases for layer 2\n",
    "        self.W2 = None # TODO: Assign the weights for layer 2\n",
    "        self.b2 = None # TODO: Assign the biases for layer 2\n",
    "\n",
    "    def relu(self, z):\n",
    "        \"\"\"\n",
    "        Applies the ReLU function element-wise:\n",
    "        ReLU(z_i) = max(0, z_i)\n",
    "        \"\"\"\n",
    "        pass # TODO: Implement the ReLU function\n",
    "\n",
    "    def softmax(self, z):\n",
    "        \"\"\"\n",
    "        Converts raw scores into probabilities:\n",
    "        z_hat = z - max(z) # stability trick\n",
    "        softmax(z_hat_i) = exp(z_hat_i) / sum(exp(z_hat_j))\n",
    "        \"\"\"\n",
    "        pass # TODO: Implement the softmax function\n",
    "\n",
    "    def linear_forward(self, W, x, b):\n",
    "        \"\"\"\n",
    "        Implements linear forward propagation: z = Wx + b\n",
    "        Args:\n",
    "            x: input vector (n,)\n",
    "            W: weight matrix (m,n)\n",
    "            b: bias vector (m,)\n",
    "        Returns:\n",
    "            z: output vector (m,)\n",
    "        \"\"\"\n",
    "        pass # TODO: Implement the linear forward function\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First layer\n",
    "        z1 = None # TODO: First layer linear transformation\n",
    "        a1 = None # TODO: First layer ReLU activation\n",
    "\n",
    "        # Output layer\n",
    "        z2 = None # TODO: Output layer linear transformation\n",
    "        y = None # TODO: Output layer softmax activation\n",
    "        return y\n",
    "    \n",
    "# Create the network\n",
    "model = SimpleFCN()\n",
    "\n",
    "# Define input vector\n",
    "x = np.array([1.0, 2.0])\n",
    "\n",
    "# Run forward pass\n",
    "output = model.forward(x)\n",
    "print(\"Output probabilities:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE SOLUTION — NOT VISIBLE TO LEARNERS\n",
    "# This cell contains the reference solution and validation.\n",
    "# It should be used for instructor/testing purposes only.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class SimpleFCN:\n",
    "    def __init__(self):\n",
    "        # Assign fixed weights and biases for layer 1\n",
    "        self.W1 = np.array([[0.2, -0.4],\n",
    "                            [0.7,  0.3]])\n",
    "        self.b1 = np.array([0.1, 0.0])\n",
    "\n",
    "        # Assign fixed weights and biases for layer 2\n",
    "        self.W2 = np.array([[ 0.5, -0.3],\n",
    "                            [-0.4,  0.6]])\n",
    "        self.b2 = np.array([0.0, 0.05])\n",
    "\n",
    "    def relu(self, z):\n",
    "        \"\"\"\n",
    "        Applies the ReLU function element-wise:\n",
    "        ReLU(z_i) = max(0, z_i)\n",
    "        \"\"\"\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def softmax(self, z):\n",
    "        \"\"\"\n",
    "        Converts raw scores into probabilities:\n",
    "        z_hat = z - max(z) # stability trick\n",
    "        softmax(z_hat_i) = exp(z_hat_i) / sum(exp(z_hat_j))\n",
    "        \"\"\"\n",
    "        exp_z = np.exp(z - np.max(z))  # stability trick\n",
    "        return exp_z / np.sum(exp_z)\n",
    "\n",
    "    def linear_forward(self, W, x, b):\n",
    "        \"\"\"\n",
    "        Implements linear forward propagation: z = Wx + b\n",
    "        Args:\n",
    "            x: input vector (n,)\n",
    "            W: weight matrix (m,n)\n",
    "            b: bias vector (m,)\n",
    "        Returns:\n",
    "            z: output vector (m,)\n",
    "        \"\"\"\n",
    "        return np.dot(W, x) + b\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First layer\n",
    "        z1 = self.linear_forward(self.W1, x, self.b1)\n",
    "        a1 = self.relu(z1)\n",
    "\n",
    "        # Output layer\n",
    "        z2 = self.linear_forward(self.W2, a1, self.b2)\n",
    "        y = self.softmax(z2)\n",
    "        return y\n",
    "    \n",
    "# Create the network\n",
    "model = SimpleFCN()\n",
    "\n",
    "# Define input vector\n",
    "x = np.array([1.0, 2.0])\n",
    "\n",
    "# Run forward pass\n",
    "output = model.forward(x)\n",
    "print(\"Output probabilities:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Your Implementation\n",
    "\n",
    "If your implementation is correct, you will be able to run this cell successfully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_validator import test_linear_forward, test_relu, test_softmax, test_forward\n",
    "\n",
    "# Run all tests\n",
    "print(\"🏁 Running all unit tests...\\n\")\n",
    "test_linear_forward(model)\n",
    "test_relu(model)\n",
    "test_softmax(model)\n",
    "test_forward(model)\n",
    "print(\"\\n🎉 All tests completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bonus: Extending the FCN to Handle Batches\n",
    "\n",
    "In this bonus task, you will extend your `SimpleFCN` class to support **batch inputs**.\n",
    "\n",
    "Currently, the network is designed to accept a single input vector $\\mathbf{x} \\in \\mathbb{R}^n$. However, in practical applications, we often process multiple inputs at once. A batch of inputs can be represented as a matrix $\\mathbf{X} \\in \\mathbb{R}^{\\text{batch\\_size} \\times n}$, where each row corresponds to a separate input example.\n",
    "\n",
    "### 👩‍💻 Your task:\n",
    "\n",
    "- Create a new class `BatchedFCN` that inherits from `SimpleFCN`.\n",
    "- Reimplement the `linear_forward()` method to support matrix inputs.\n",
    "- Modify the `forward()` method to handle a batch of inputs.\n",
    "- Ensure that all operations — linear transformations, activations, and softmax — work correctly across the batch dimension.\n",
    "\n",
    "> 💡 Depending on how you implemented the `linear_forward` method in `SimpleFCN`, your code may already generalize to batch inputs. However, for clarity and structure, you are encouraged to explicitly adapt and test it in this context.\n",
    "\n",
    "You may assume that the input is a NumPy array of shape `(batch_size, input_dim)`, and the output should be of shape `(batch_size, output_dim)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BatchedFCN(SimpleFCN):\n",
    "    def linear_forward(self, W, X, b):\n",
    "        \"\"\"\n",
    "        Computes the linear transformation Z = XWᵀ + b for a batch of inputs.\n",
    "        Parameters:\n",
    "            W: weight matrix of shape (output_dim, input_dim)\n",
    "            X: input matrix of shape (batch_size, input_dim)\n",
    "            b: bias vector of shape (output_dim,)\n",
    "        Returns:\n",
    "            Z: output matrix of shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        # TODO: Implement the batched linear transformation\n",
    "        Z = None # your code here\n",
    "        return Z\n",
    "\n",
    "    def forward(self, X_batch):\n",
    "        \"\"\"\n",
    "        Forward pass for a batch of inputs.\n",
    "        Parameters:\n",
    "            X_batch: shape (batch_size, input_dim)\n",
    "        Returns:\n",
    "            Y: output probabilities for each input in the batch,\n",
    "               shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        # TODO: Reuse linear_forward, ReLU, and softmax for a batch\n",
    "        z1 = None # your code here\n",
    "        a1 = None # your code here\n",
    "        z2 = None # your code here\n",
    "        Y = None # your code here\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE SOLUTION — NOT VISIBLE TO LEARNERS\n",
    "# This cell contains the reference solution and validation.\n",
    "# It should be used for instructor/testing purposes only.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class BatchedFCN(SimpleFCN):\n",
    "    def linear_forward(self, W, X, b):\n",
    "        \"\"\"\n",
    "        Computes the linear transformation Z = XWᵀ + b for a batch of inputs.\n",
    "        Parameters:\n",
    "            W: weight matrix of shape (output_dim, input_dim)\n",
    "            X: input matrix of shape (batch_size, input_dim)\n",
    "            b: bias vector of shape (output_dim,)\n",
    "        Returns:\n",
    "            Z: output matrix of shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        # TODO: Implement the batched linear transformation\n",
    "        Z = np.dot(X,W.transpose())+b # your code here\n",
    "        return Z\n",
    "\n",
    "    def forward(self, X_batch):\n",
    "        \"\"\"\n",
    "        Forward pass for a batch of inputs.\n",
    "        Parameters:\n",
    "            X_batch: shape (batch_size, input_dim)\n",
    "        Returns:\n",
    "            Y: output probabilities for each input in the batch,\n",
    "               shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        # TODO: Reuse linear_forward, ReLU, and softmax for a batch\n",
    "        z1 = self.linear_forward(self.W1,X_batch,self.b1) # your code here\n",
    "        a1 = self.relu(z1) # your code here\n",
    "        z2 = self.linear_forward(self.W2,X_batch,self.b2) # your code here\n",
    "        Y = np.apply_along_axis(self.softmax, axis=1, arr=z2) # your code here\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Your Implementation\n",
    "\n",
    "If your implementation is correct, you will be able to run this cell successfully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_validator import test_batched_forward\n",
    "\n",
    "model = BatchedFCN()\n",
    "test_batched_forward(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
